{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Processing Results:\n",
      "\n",
      "Test Case 1:\n",
      "Sentences: ['Bitcoin is a cryptocurrency invented in 2008 by an unknown person or group of people using the name Satoshi Nakamoto.', ' The currency began use in 2009 when its implementation was released as open-source software.']\n",
      "Words: ['Bitcoin', 'is', 'a', 'cryptocurrency', 'invented', 'in', '2008', 'by', 'an', 'unknown', 'person', 'or', 'group', 'of', 'people', 'using', 'the', 'name', 'Satoshi', 'Nakamoto', '.', 'The', 'currency', 'began', 'use', 'in', '2009', 'when', 'its', 'implementation', 'was', 'released', 'as', 'open', '-', 'source', 'software', '.']\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_text(text):\n",
    "    word_tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]+')\n",
    "    word_tokens = word_tokenizer.tokenize(text)\n",
    "    \n",
    "    sent_tokenizer = RegexpTokenizer(r'[^.!?]+[.!?]')\n",
    "    sent_tokens = sent_tokenizer.tokenize(text)\n",
    "    \n",
    "    analysis = {\n",
    "        'original_text': text,\n",
    "        'word_tokens': word_tokens,\n",
    "        'sentence_tokens': sent_tokens,\n",
    "        'word_count': len(word_tokens),\n",
    "        'sentence_count': len(sent_tokens)\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "test_cases = [\n",
    "    \"\"\"Bitcoin is a cryptocurrency invented in 2008 by an unknown person or group of people using the name Satoshi Nakamoto. The currency began use in 2009 when its implementation was released as open-source software.\"\"\"\n",
    "]\n",
    "\n",
    "print(\"Text Processing Results:\\n\")\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    analysis = analyze_text(test)\n",
    "    \n",
    "    print(f\"Test Case {i}:\")\n",
    "    print(f\"Sentences: {analysis['sentence_tokens']}\")\n",
    "    print(f\"Words: {analysis['word_tokens']}\")\n",
    "    print('-' * 70 + '\\n')\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Word Count': analyze_text(text)['word_count'],\n",
    "        'Sentence Count': analyze_text(text)['sentence_count']\n",
    "    }\n",
    "    for text in test_cases\n",
    "])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArtificialIntelligence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
